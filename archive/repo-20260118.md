## repo.md

```markdown
<repo>
  <system_overview>
    This repo defines the ThinkPad T480 as the control-plane workstation for a small Fedora/RHEL lab.
    The lab includes an Asahi-based server (srv-m1m) as the primary services and data hub, plus additional nodes such as raspi4.
    The focus is senior-level, interview-defensible Linux infrastructure, with automation and documentation kept in sync.
  </system_overview>

  <device_t480>
    <role>
      Primary admin and development workstation.
      Control plane for Ansible, CI orchestration, and documentation authoring.
    </role>
    <paths>
      <ansible>ansible/</ansible>
      <runbooks>runbooks/</runbooks>
      <adr>ADR/</adr>
      <audits>audits/</audits>
      <ci>ci/</ci>
      <scripts>scripts/</scripts>
      <system>t480-system/</system>
      <troubleshoot>troubleshoot/</troubleshoot>
    </paths>
    <baseline>
      Golden state v1 for t480 is defined in Ansible playbooks (for example, t480-baseline.yml) and validated by local scripts under scripts/.
      Audits and system specs under audits/ and t480-system/ describe the expected package set, services, and security posture for the control-plane workstation.
      Validation scripts (such as t480-baseline-validate.sh) assert the presence of required packages, binaries, and services to keep t480 in a reproducible state.
    </baseline>
    <notes>
      All operational changes should be initiated and recorded from the T480 where possible.
      This repo is the source of truth for how T480 participates in and manages the lab.
      Day-to-day, t480 is used to run Ansible, manage CI, and edit documentation that describes srv-m1m and other nodes.
    </notes>
  </device_t480>

  <device_m1m>
    <role>
      Primary services and data node, running core lab services (Forgejo, CI, databases, monitoring, secrets).
      Target for most Ansible-driven deployments and service configurations initiated from T480.
    </role>
    <services>
      <git>Forgejo (Git hosting)</git>
      <ci>Woodpecker CI</ci>
      <database>PostgreSQL</database>
      <monitoring>Prometheus, Grafana</monitoring>
      <secrets>Vaultwarden</secrets>
      <other>Additional app services, typically via containers</other>
    </services>
    <storage>
      /mnt/asahi is the primary data and services mount for srv-m1m.
      Container volumes, databases, configuration, and other service data for Forgejo, Woodpecker, PostgreSQL, Prometheus/Grafana, Vaultwarden, and related workloads are stored under /mnt/asahi in a structured layout.
      The goal is to keep application data and configuration separate from the root filesystem to simplify backups, restores, and hardware replacement.
    </storage>
    <runtime>
      Core services on srv-m1m are expected to run in rootless containers under a non-root user.
      Container definitions, compose files, or equivalent automation are managed from this repo and applied from t480.
      Service health, metrics, and logs are intended to be visible from t480 via monitoring and observability tooling.
    </runtime>
    <notes>
      Git hosting on srv-m1m is intended to be authoritative for lab infrastructure repos.
      External services (for example, GitHub) may exist as mirrors but are not the primary source of truth.
      srv-m1m acts as the main services and backup hub; additional nodes and experiments should integrate with it rather than bypass it.
    </notes>
  </device_m1m>

  <other_devices>
    <raspi4>
      <role>
        Auxiliary lab node for monitoring targets, experiments, or edge workloads.
      </role>
      <management>
        Managed from T480 via Ansible and, where applicable, CI-driven workflows.
      </management>
    </raspi4>
    <containers>
      <role>
        Containerized workloads used for services and experiments, primarily hosted on srv-m1m and other container-capable hosts.
      </role>
      <notes>
        Containers should read and write data under well-defined directories (for example, under /mnt/asahi on srv-m1m) instead of storing important state inside container layers.
      </notes>
    </containers>
    <backup_targets>
      <role>
        Nodes or services that receive backups (local disks, NAS, cloud storage).
      </role>
      <notes>
        Backup targets are expected to integrate with srv-m1m and t480 workflows, with configuration and runbooks captured in this repo.
      </notes>
    </backup_targets>
  </other_devices>

  <conventions>
    <hostnames>
      T480 is referred to as "t480".
      The Asahi server is referred to as "srv-m1m".
      Additional nodes should use clear, role-indicative hostnames.
    </hostnames>
    <files>
      Runbooks follow the pattern "RUNBOOK-*.md" under runbooks/.
      ADR files live under ADR/ and follow a consistent naming pattern encoding date and subject.
      Audit directories under audits/ are timestamped and may contain raw command outputs and summaries.
    </files>
    <documentation>
      README.md is the primary human entry point.
      ARCHITECTURE.md is the source of truth for system layout and workflows.
      This repo.md file is optimized for AI tools needing structured context about hosts, roles, paths, and governance.
    </documentation>
  </conventions>

  <governance>
    <charter>
      T480-CHARTER.md defines the project charter, workflow rules, and AIâ€“human interaction contract for this repo.
    </charter>
    <expectations>
      Do not assume missing details; request clarification or pointers to specific files.
      When making changes, update relevant runbooks, ADRs, and this repo when necessary.
      All material infrastructure changes should be traceable via Git history and, where appropriate, ADRs.
      Automation and documentation should evolve together so that t480, srv-m1m, and other nodes remain understandable and operable by humans without external context.
    </expectations>
  </governance>
</repo>

