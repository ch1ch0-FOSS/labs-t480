## repo.md

```markdown
<repo>
  <system_overview>
    This repo defines the ThinkPad T480 as the control-plane workstation for a small Fedora/RHEL lab.
    The lab includes an Asahi-based server (srv-m1m) as the primary services and data hub, plus additional nodes such as raspi4.
    The focus is senior-level, interview-defensible Linux infrastructure, with automation and documentation kept in sync.
  </system_overview>

  <device_t480>
    <role>
      Primary admin and development workstation.
      Control plane for Ansible, CI orchestration, and documentation authoring.
    </role>
    <paths>
      <ansible>ansible/</ansible>
      <runbooks>runbooks/</runbooks>
      <adr>ADR/</adr>
      <audits>audits/</audits>
      <ci>ci/</ci>
      <scripts>scripts/</scripts>
      <system>t480-system/</system>
      <troubleshoot>troubleshoot/</troubleshoot>
    </paths>
    <baseline>
      Golden state for t480 is defined in Ansible playbooks (for example, t480-baseline.yml) and validated by local scripts under scripts/.
      Audits and system specs under audits/ and t480-system/ describe the expected package set, services, and security posture for the control-plane workstation.
      Validation scripts (such as t480-baseline-validate.sh) assert the presence of required packages, binaries, and services to keep t480 in a reproducible state.
    </baseline>
    <notes>
      All operational changes should be initiated and recorded from the T480 where possible.
      This repo is the source of truth for how T480 participates in and manages the lab.
      Day-to-day, t480 is used to run Ansible, manage CI, and edit documentation that describes srv-m1m and other nodes.
    </notes>
  </device_t480>

  <device_m1m>
    <role>
      Primary services and data node, running core lab services (Forgejo, CI, databases, monitoring, secrets).
      Target for most Ansible-driven deployments and service configurations initiated from T480.
    </role>

    <services>
      <git>Forgejo (Git hosting)</git>
      <ci>Woodpecker CI (planned)</ci>
      <database>PostgreSQL</database>
      <monitoring>Prometheus, Grafana (planned)</monitoring>
      <secrets>Vaultwarden</secrets>
      <other>Additional app services, typically via systemd units or containers</other>
    </services>

    <storage>
      /mnt/data is the primary capacity-tier data and services mount for srv-m1m.
      /srv is a bind mount of /mnt/data/srv and is the canonical service root for Forgejo, Vaultwarden, and future services that should live on the capacity tier.
      Service data is expected to live under /mnt/data/srv/&lt;service&gt; (for example, /mnt/data/srv/forgejo and /mnt/data/srv/vaultwarden), presented to the system as /srv/&lt;service&gt;.
      PostgreSQL currently uses its default OS data directory (for example, /var/lib/pgsql/data); migration to a performance tier such as /mnt/fastdata is deferred to a dedicated playbook and ADR.
    </storage>

    <runtime>
      Core services on srv-m1m are currently provided as systemd services:
        - postgresql.service
        - forgejo.service (or equivalent Forgejo unit)
        - vaultwarden.service
        - node_exporter.service
      The srv-m1m baseline Ansible playbook (srv-m1m-baseline.yml) is responsible for:
        - Validating Btrfs snapshots used as safety nets before structural changes.
        - Ensuring /srv is bind-mounted from /mnt/data/srv and required service directories exist.
        - Normalizing ownership and layout for Forgejo and Vaultwarden data under /mnt/data/srv.
        - Installing and maintaining the vaultwarden.service unit with WorkingDirectory=/srv/vaultwarden and EnvironmentFile=/etc/vaultwarden/vaultwarden.env.
        - Ensuring the above services are enabled and in the started state after a baseline run.
      Rootless containers and more complex orchestration are considered future enhancements and are not assumed in the current baseline.
    </runtime>

    <notes>
      Git hosting on srv-m1m (Forgejo) is intended to be authoritative for lab infrastructure repos.
      External services (for example, GitHub) may exist as mirrors but are not the primary source of truth.
      srv-m1m acts as the main services and backup hub; additional nodes and experiments should integrate with it rather than bypass it.
      Higher-risk changes such as PostgreSQL data relocation to /mnt/fastdata are handled by dedicated, one-shot playbooks and ADRs, not by the general baseline.
    </notes>
  </device_m1m>

  <other_devices>
    <raspi4>
      <role>
        Auxiliary lab node for monitoring targets, experiments, or edge workloads.
      </role>
      <management>
        Managed from T480 via Ansible and, where applicable, CI-driven workflows.
      </management>
    </raspi4>

    <containers>
      <role>
        Containerized workloads used for services and experiments, potentially hosted on srv-m1m and other container-capable hosts.
      </role>
      <notes>
        Containers should read and write data under well-defined directories (for example, under /mnt/data/srv on srv-m1m) instead of storing important state inside container layers.
        Where containers are introduced, their configuration should be driven from this repo (compose files, systemd units, or Ansible roles) and integrated with the existing storage layout.
      </notes>
    </containers>

    <backup_targets>
      <role>
        Nodes or services that receive backups (local disks, NAS, cloud storage).
      </role>
      <notes>
        Backup targets are expected to integrate with srv-m1m and t480 workflows, with configuration and runbooks captured in this repo.
        Btrfs snapshots on /mnt/data and /mnt/fastdata are used as local safety nets; longer-term backups should be described under runbooks/ and ADR/.
      </notes>
    </backup_targets>
  </other_devices>

  <conventions>
    <hostnames>
      T480 is referred to as "t480".
      The Asahi server is referred to as "srv-m1m".
      Additional nodes should use clear, role-indicative hostnames.
    </hostnames>

    <files>
      Runbooks follow the pattern "RUNBOOK-*.md" under runbooks/.
      ADR files live under ADR/ and follow a consistent naming pattern encoding date and subject.
      Audit directories under audits/ are timestamped and may contain raw command outputs and summaries.
    </files>

    <documentation>
      README.md is the primary human entry point.
      ARCHITECTURE.md is the source of truth for system layout and workflows.
      This repo.md file is optimized for AI tools needing structured context about hosts, roles, paths, and governance, and is kept in sync with the current srv-m1m baseline.
    </documentation>
  </conventions>

  <governance>
    <charter>
      T480-CHARTER.md defines the project charter, workflow rules, and AIâ€“human interaction contract for this repo.
    </charter>
    <expectations>
      Do not assume missing details; request clarification or pointers to specific files.
      When making changes, update relevant runbooks, ADRs, and this repo when necessary.
      All material infrastructure changes should be traceable via Git history and, where appropriate, ADRs.
      Automation and documentation should evolve together so that t480, srv-m1m, and other nodes remain understandable and operable by humans without external context.
    </expectations>
  </governance>
</repo>

