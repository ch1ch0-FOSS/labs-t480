## repo.md

```markdown
<repo>
  <system_overview>
    This repo defines the ThinkPad T480 as the control-plane workstation for a small Fedora/RHEL lab.
    The lab includes an Asahi-based server (srv-m1m) as the primary services and data hub, plus additional nodes such as raspi4.
    The focus is senior-level, interview-defensible Linux infrastructure, with automation and documentation kept in sync.
    All infrastructure changes, documentation updates, and experiments should be initiated from the T480 using Linux CLI tools (Ansible, systemd, podman/containers, ssh) and the nb notebook system for documentation.[1]
  </system_overview>

  <device_t480>
    <role>
      Primary admin and development workstation.
      Control plane for Ansible, CI orchestration, and documentation authoring.
      Source of truth for nb-based documentation and CLI-driven workflows across all devices.[1]
    </role>
    <paths>
      <ansible>ansible/</ansible>
      <runbooks>runbooks/</runbooks>
      <adr>ADR/</adr>
      <audits>audits/</audits>
      <ci>ci/</ci>
      <scripts>scripts/</scripts>
      <system>t480-system/</system>
      <troubleshoot>troubleshoot/</troubleshoot>
      <archive>archive/</archive>
    </paths>
    <baseline>
      Golden state for t480 is defined in Ansible playbooks (for example, t480-baseline.yml) and validated by local scripts under scripts/.
      Audits and system specs under audits/ and t480-system/ describe the expected package set, services, and security posture for the control-plane workstation.
      Validation scripts (such as t480-baseline-validate.sh) assert the presence of required packages, binaries, and services to keep t480 in a reproducible state.
      Daily operations, ADR drafting, incident writeups, and runbook authoring are performed from the T480 using nb notebooks and standard Linux tools (vim, git, ssh, systemctl, podman).[1]
    </baseline>
    <notes>
      All operational changes should be initiated and recorded from the T480 where possible.
      This repo is the source of truth for how T480 participates in and manages the lab.
      Day-to-day, t480 is used to:
        - run Ansible against srv-m1m and other nodes,
        - manage CI configurations,
        - edit documentation (ADRs, runbooks, logs) using nb and vim,
        - collect audits and troubleshooting outputs via CLI tools.
      Direct changes on srv-m1m or other nodes should be rare and must be captured after the fact in this repo and in nb.[1]
    </notes>

    <nb_workflow>
      <notebooks>
        t480: control-plane ADRs, logs, and MOC.
        srv-m1m: services node ADRs, incidents, and MOC.
        linux-pro: RHEL/LPIC study notes and runbooks.
        toolkit: local docs for tools (nb, vim, w3m, vimb, etc.).
        inbox: todos and scratch.[2]
      </notebooks>
      <mocs>
        Each notebook has a Map of Contents note (MOC) acting as an operational dashboard.
        MOCs are hand-curated, Zettelkasten-style hub notes:
          - Context for the notebook's domain.
          - Command-first discovery snippets (nb ls, nb daily, etc.).
          - Key recent decisions, incidents, and promoted artifacts.
        Automation for MOCs is intentionally avoided to preserve nb’s pinning and browsing behavior.[3][4][5]
      </mocs>
      <promotion>
        ADRs, runbooks, and other durable artifacts are drafted in nb and promoted into this repo when they stabilize:
          - scripts/nb-promote.sh promotes nb ADRs into ADR/ADR-<NNN>-*.md on a short-lived branch.
          - runbooks/RUNBOOK-nb-to-repo-promotion.md documents the promotion process and criteria.
        nb notes remain the working area; ADR/ and runbooks/ store curated, interview-ready documents.[6][7][8][9]
      </promotion>
      <backup>
        The t480 nb notebook (~/.nb/t480) is considered critical data.
        An ADR in nb defines a backup strategy using a local bare Git repo under ~/Sync/nb-backups/t480.git.
        A runbook (RUNBOOK-nb-backup-restore.md) will describe manual backup and restore procedures and may later be extended to push to srv-m1m/Forgejo once the services node is re-aligned via Ansible.[4]
      </backup>
    </nb_workflow>
  </device_t480>

  <device_m1m>
    <role>
      Primary services and data node, running core lab services (Forgejo, CI, databases, monitoring, secrets).
      Target for most Ansible-driven deployments and service configurations initiated from T480.
      Operated primarily via SSH and Ansible from the T480, not by interactive local configuration.[1]
    </role>

    <services>
      <git>Forgejo (Git hosting)</git>
      <ci>Woodpecker CI (planned)</ci>
      <database>PostgreSQL</database>
      <monitoring>Prometheus, Grafana (planned)</monitoring>
      <secrets>Vaultwarden</secrets>
      <other>Additional app services, typically via systemd units or containers</other>
    </services>

    <storage>
      /mnt/data is the primary capacity-tier data and services mount for srv-m1m.
      /srv is a bind mount of /mnt/data/srv and is the canonical service root for Forgejo, Vaultwarden, and future services that should live on the capacity tier.
      Service data is expected to live under /mnt/data/srv/&lt;service&gt; (for example, /mnt/data/srv/forgejo and /mnt/data/srv/vaultwarden), presented to the system as /srv/&lt;service&gt;.
      PostgreSQL currently uses its default OS data directory (for example, /var/lib/pgsql/data); migration to a performance tier such as /mnt/fastdata is deferred to a dedicated playbook and ADR.[1]
    </storage>

    <runtime>
      Core services on srv-m1m are currently provided as systemd services:
        - postgresql.service
        - forgejo.service (or equivalent Forgejo unit)
        - vaultwarden.service
        - node_exporter.service
      The srv-m1m baseline Ansible playbook (srv-m1m-baseline.yml) is responsible for:
        - Validating Btrfs snapshots used as safety nets before structural changes.
        - Ensuring /srv is bind-mounted from /mnt/data/srv and required service directories exist.
        - Normalizing ownership and layout for Forgejo and Vaultwarden data under /mnt/data/srv.
        - Installing and maintaining the vaultwarden.service unit with WorkingDirectory=/srv/vaultwarden and EnvironmentFile=/etc/vaultwarden/vaultwarden.env.
        - Ensuring the above services are enabled and in the started state after a baseline run.
      Rootless containers and more complex orchestration are considered future enhancements and are not assumed in the current baseline.
      All of these operations should be driven from the T480 using:
        - ssh t480 → srv-m1m
        - ansible-playbook against the srv-m1m inventory
        - systemctl, journalctl, podman, and standard Linux tools executed via SSH or Ansible tasks.[1]
    </runtime>
  </device_m1m>

  <other_devices>
    <raspi4>
      <role>
        Auxiliary lab node for monitoring targets, experiments, or edge workloads.
      </role>
      <management>
        Managed from T480 via Ansible and, where applicable, CI-driven workflows.
        Direct manual changes on the device should be rare and followed by documentation updates in nb and this repo.[1]
      </management>
    </raspi4>

    <containers>
      <role>
        Containerized workloads used for services and experiments, potentially hosted on srv-m1m and other container-capable hosts.
      </role>
      <notes>
        Containers should read and write data under well-defined directories (for example, under /mnt/data/srv on srv-m1m) instead of storing important state inside container layers.
        Where containers are introduced, their configuration should be driven from this repo (compose files, systemd units, or Ansible roles) and integrated with the existing storage layout.
        Container lifecycle (build, run, updates) should be controlled from the T480 via CLI (podman/docker, Ansible modules) and documented in nb runbooks.[1]
      </notes>
    </containers>

    <backup_targets>
      <role>
        Nodes or services that receive backups (local disks, NAS, cloud storage).
      </role>
      <notes>
        Backup targets are expected to integrate with srv-m1m and t480 workflows, with configuration and runbooks captured in this repo.
        Btrfs snapshots on /mnt/data and /mnt/fastdata are used as local safety nets; longer-term backups should be described under runbooks/ and ADR/.
        Backup and restore procedures should be executed and tested from the T480 using CLI tools (rsync, btrfs, borg/restic, etc.) and recorded in nb logs and runbooks.[1]
      </notes>
    </backup_targets>
  </other_devices>

  <conventions>
    <hostnames>
      T480 is referred to as "t480".
      The Asahi server is referred to as "srv-m1m".
      Additional nodes should use clear, role-indicative hostnames.[1]
    </hostnames>

    <files>
      Runbooks follow the pattern "RUNBOOK-*.md" under runbooks/.
      ADR files live under ADR/ and follow a consistent naming pattern encoding sequence and subject.
      Audit directories under audits/ are timestamped and may contain raw command outputs and summaries.
      nb notebooks (t480, srv-m1m, linux-pro, toolkit, inbox) live on the T480 and are considered the first place where changes are drafted and logged.[2][1]
    </files>

    <documentation>
      README.md is the primary human entry point.
      ARCHITECTURE.md is the source of truth for system layout and workflows.
      This repo.md file is optimized for AI tools needing structured context about hosts, roles, paths, workflows, and governance, and is kept in sync with the current T480 and srv-m1m baselines.
      nb is the primary local documentation system; key ADRs and runbooks are promoted from nb into ADR/ and runbooks/ in this repo for long-term tracking and sharing.[10][1]
    </documentation>
  </conventions>

  <governance>
    <charter>
      T480-CHARTER.md defines the project charter, workflow rules, and AI–human interaction contract for this repo.[11]
    </charter>
    <expectations>
      Do not assume missing details; request clarification or pointers to specific files.
      When making changes, update relevant runbooks, ADRs, nb notes, and this repo when necessary.
      All material infrastructure changes should be traceable via Git history and, where appropriate, ADRs.
      Automation and documentation should evolve together so that t480, srv-m1m, and other nodes remain understandable and operable by humans without external context.
      The preferred workflow is:
        - Plan and record the change in nb (ADR/log) on the T480.
        - Implement the change via CLI tools (Ansible, systemctl, podman, etc.) from the T480.
        - Validate and capture outputs (audit scripts, commands) under this repo.[2][1]
    </expectations>
  </governance>
</repo>

