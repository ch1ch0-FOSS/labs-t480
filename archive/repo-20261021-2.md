## repo.md

```markdown
<repo>
  <system_overview>
    This repo defines the ThinkPad T480 as the control-plane workstation for a small Fedora/RHEL lab.
    The lab includes an Asahi-based server (srv-m1m) as the primary services and data hub, plus additional nodes such as raspi4.
    The focus is senior-level, interview-defensible Linux infrastructure, with automation and documentation kept in sync.
    All infrastructure changes, documentation updates, and experiments should be initiated from the T480 using Linux CLI tools (Ansible, systemd, podman/containers, ssh) and the nb notebook system for documentation.
  </system_overview>

  <device_t480>
    <role>
      Primary admin and development workstation.
      Control plane for Ansible, CI orchestration, and documentation authoring.
      Source of truth for nb-based documentation and CLI-driven workflows across all devices.
    </role>
    <paths>
      <ansible>ansible/</ansible>
      <runbooks>runbooks/</runbooks>
      <adr>ADR/</adr>
      <audits>audits/</audits>
      <ci>ci/</ci>
      <scripts>scripts/</scripts>
      <system>t480-system/</system>
      <troubleshoot>troubleshoot/</troubleshoot>
    </paths>
    <baseline>
      Golden state for t480 is defined in Ansible playbooks (for example, t480-baseline.yml) and validated by local scripts under scripts/.
      Audits and system specs under audits/ and t480-system/ describe the expected package set, services, and security posture for the control-plane workstation.
      Validation scripts (such as t480-baseline-validate.sh) assert the presence of required packages, binaries, and services to keep t480 in a reproducible state.
      Daily operations, ADR drafting, incident writeups, and runbook authoring are performed from the T480 using nb notebooks and standard Linux tools (vim, git, ssh, systemctl, podman).
    </baseline>
    <notes>
      All operational changes should be initiated and recorded from the T480 where possible.
      This repo is the source of truth for how T480 participates in and manages the lab.
      Day-to-day, t480 is used to:
        - run Ansible against srv-m1m and other nodes,
        - manage CI configurations,
        - edit documentation (ADRs, runbooks, logs) using nb and vim,
        - collect audits and troubleshooting outputs via CLI tools.
      Direct changes on srv-m1m or other nodes should be rare and must be captured after the fact in this repo and in nb.
    </notes>
  </device_t480>

  <device_m1m>
    <role>
      Primary services and data node, running core lab services (Forgejo, CI, databases, monitoring, secrets).
      Target for most Ansible-driven deployments and service configurations initiated from T480.
      Operated primarily via SSH and Ansible from the T480, not by interactive local configuration.
    </role>

    <services>
      <git>Forgejo (Git hosting)</git>
      <ci>Woodpecker CI (planned)</ci>
      <database>PostgreSQL</database>
      <monitoring>Prometheus, Grafana (planned)</monitoring>
      <secrets>Vaultwarden</secrets>
      <other>Additional app services, typically via systemd units or containers</other>
    </services>

    <storage>
      /mnt/data is the primary capacity-tier data and services mount for srv-m1m.
      /srv is a bind mount of /mnt/data/srv and is the canonical service root for Forgejo, Vaultwarden, and future services that should live on the capacity tier.[3][4]
      Service data is expected to live under /mnt/data/srv/&lt;service&gt; (for example, /mnt/data/srv/forgejo and /mnt/data/srv/vaultwarden), presented to the system as /srv/&lt;service&gt;.[4]
      PostgreSQL currently uses its default OS data directory (for example, /var/lib/pgsql/data); migration to a performance tier such as /mnt/fastdata is deferred to a dedicated playbook and ADR.[5][4]
    </storage>

    <runtime>
      Core services on srv-m1m are currently provided as systemd services:
        - postgresql.service
        - forgejo.service (or equivalent Forgejo unit)
        - vaultwarden.service
        - node_exporter.service
      The srv-m1m baseline Ansible playbook (srv-m1m-baseline.yml) is responsible for:
        - Validating Btrfs snapshots used as safety nets before structural changes.
        - Ensuring /srv is bind-mounted from /mnt/data/srv and required service directories exist.
        - Normalizing ownership and layout for Forgejo and Vaultwarden data under /mnt/data/srv.
        - Installing and maintaining the vaultwarden.service unit with WorkingDirectory=/srv/vaultwarden and EnvironmentFile=/etc/vaultwarden/vaultwarden.env.[4]
        - Ensuring the above services are enabled and in the started state after a baseline run.
      Rootless containers and more complex orchestration are considered future enhancements and are not assumed in the current baseline.[6]
      All of these operations should be driven from the T480 using:
        - ssh t480 → srv-m1m
        - ansible-playbook against the srv-m1m inventory
        - systemctl, journalctl, podman, and standard Linux tools executed via SSH or Ansible tasks.
    </runtime>

    <current_state>
      srv-m1m is running core services as systemd units using the standardized storage layout:
        - postgresql.service with data in the default distro path (for example, /var/lib/pgsql/data).[5][4]
        - forgejo.service using data under /mnt/data/srv/forgejo, expected to be the primary Git hosting for lab repos.[4]
        - vaultwarden.service with WorkingDirectory=/srv/vaultwarden and EnvironmentFile=/etc/vaultwarden/vaultwarden.env.[4]
        - node_exporter.service for basic metrics scraping.
      The filesystem layout is normalized so that:
        - /mnt/data is the capacity-tier data and services mount.
        - /srv is a bind mount of /mnt/data/srv and is the canonical service root path for capacity-tier services.[3][4]
      PostgreSQL migration to /mnt/fastdata (performance tier) is explicitly deferred and must be handled by dedicated migration playbooks and ADRs, not by the general baseline.[5][4]
      Forgejo and Vaultwarden configurations are expected to be expressed as systemd units, Ansible roles, and nb-documented runbooks, all edited from the T480.
    </current_state>

    <next_steps>
      Planned enhancements for srv-m1m include:
        - Introducing Woodpecker CI, Prometheus, and Grafana using the same /srv + /mnt/data pattern, with configuration driven from this repo and changes applied from the T480 via Ansible.[6]
        - Authoring and applying a dedicated PostgreSQL migration playbook to move data to /mnt/fastdata with clear rollback and validation steps, executed from the T480.
        - Expanding the srv-m1m-baseline.yml playbook as new services become stable so that a single baseline run converges services, storage, users, observability, and basic hardening.[4]
        - Adding ADRs under ADR/ that describe storage layout decisions, PostgreSQL migration strategy, and the T480 control-plane role in more detail.[3][5][4]
        - Capturing each significant change as:
          - an nb log and/or ADR in the `srv-m1m` and `t480` notebooks, and
          - a corresponding update in this repo (ADR, runbook, or script).
    </next_steps>

    <issues_recent>
      Recent issues and lessons learned on srv-m1m:
        - A systemd override that redirected PostgreSQL data to /mnt/fastdata caused SELinux and permission-related startup failures; this was resolved by removing the override, restoring ownership and contexts, and returning PostgreSQL to its default data directory.[5][4]
        - SELinux labeling on experimental PostgreSQL paths underscored the need to keep high-risk storage changes out of baselines and to treat them as explicit, one-off migrations with documented rollback.[5][4]
        - Legacy fstab entries for MySQL and a bind-mounted /home/ch1ch0 were cleaned up to avoid dependence on obsolete layouts and to enforce the new /mnt/data + /srv storage model.[4][5]
      These incidents are documented via nb incident/log notes and ADRs, and summarized here so that future experiments re-use established runbooks instead of ad-hoc shell edits.
    </issues_recent>

    <notes>
      Git hosting on srv-m1m (Forgejo) is intended to be authoritative for lab infrastructure repos.
      External services (for example, GitHub) may exist as mirrors but are not the primary source of truth.
      srv-m1m acts as the main services and backup hub; additional nodes and experiments should integrate with it rather than bypass it.
      Higher-risk changes such as PostgreSQL data relocation to /mnt/fastdata are handled by dedicated, one-shot playbooks and ADRs, not by the general baseline.
      All such changes should be planned and executed from the T480 using:
        - nb for ADRs, logs, and runbooks,
        - ansible-playbook and shell commands for implementation,
        - git for tracking changes in this repo.
    </notes>
  </device_m1m>

  <other_devices>
    <raspi4>
      <role>
        Auxiliary lab node for monitoring targets, experiments, or edge workloads.
      </role>
      <management>
        Managed from T480 via Ansible and, where applicable, CI-driven workflows.
        Direct manual changes on the device should be rare and followed by documentation updates in nb and this repo.
      </management>
    </raspi4>

    <containers>
      <role>
        Containerized workloads used for services and experiments, potentially hosted on srv-m1m and other container-capable hosts.
      </role>
      <notes>
        Containers should read and write data under well-defined directories (for example, under /mnt/data/srv on srv-m1m) instead of storing important state inside container layers.
        Where containers are introduced, their configuration should be driven from this repo (compose files, systemd units, or Ansible roles) and integrated with the existing storage layout.
        Container lifecycle (build, run, updates) should be controlled from the T480 via CLI (podman/docker, Ansible modules) and documented in nb runbooks.
      </notes>
    </containers>

    <backup_targets>
      <role>
        Nodes or services that receive backups (local disks, NAS, cloud storage).
      </role>
      <notes>
        Backup targets are expected to integrate with srv-m1m and t480 workflows, with configuration and runbooks captured in this repo.
        Btrfs snapshots on /mnt/data and /mnt/fastdata are used as local safety nets; longer-term backups should be described under runbooks/ and ADR/.
        Backup and restore procedures should be executed and tested from the T480 using CLI tools (rsync, btrfs, borg/restic, etc.) and recorded in nb logs and runbooks.
      </notes>
    </backup_targets>
  </other_devices>

  <conventions>
    <hostnames>
      T480 is referred to as "t480".
      The Asahi server is referred to as "srv-m1m".
      Additional nodes should use clear, role-indicative hostnames.
    </hostnames>

    <files>
      Runbooks follow the pattern "RUNBOOK-*.md" under runbooks/.
      ADR files live under ADR/ and follow a consistent naming pattern encoding date and subject.
      Audit directories under audits/ are timestamped and may contain raw command outputs and summaries.
      nb notebooks (t480, srv-m1m, linux-pro, toolkit, inbox) live on the T480 and are considered the first place where changes are drafted and logged.
    </files>

    <documentation>
      README.md is the primary human entry point.
      ARCHITECTURE.md is the source of truth for system layout and workflows.[6]
      This repo.md file is optimized for AI tools needing structured context about hosts, roles, paths, and governance, and is kept in sync with the current srv-m1m baseline.[1]
      nb is the primary local documentation system; key ADRs and runbooks are promoted from nb into ADR/ and runbooks/ in this repo for long-term tracking and sharing.
    </documentation>
  </conventions>

  <governance>
    <charter>
      T480-CHARTER.md defines the project charter, workflow rules, and AI–human interaction contract for this repo.
    </charter>
    <expectations>
      Do not assume missing details; request clarification or pointers to specific files.
      When making changes, update relevant runbooks, ADRs, nb notes, and this repo when necessary.
      All material infrastructure changes should be traceable via Git history and, where appropriate, ADRs.
      Automation and documentation should evolve together so that t480, srv-m1m, and other nodes remain understandable and operable by humans without external context.
      The preferred workflow is:
        - Plan and record the change in nb (ADR/log) on the T480.
        - Implement the change via CLI tools (Ansible, systemctl, podman, etc.) from the T480.
        - Validate and capture outputs (audit scripts, commands) under this repo.
    </expectations>
  </governance>
</repo>

